{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkielo3/mammalian_brains/blob/main/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JR3Ptn-SFYRJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy pandas scipy scikit-learn matplotlib seaborn pillow ipykernel\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q altair datasets\n",
        "!pip install -q numba numbasom==0.0.5\n",
        "!pip install -q tensorboard\n",
        "\n",
        "!git clone https://github.com/mkielo3/mammalian_brains.git\n",
        "%cd mammalian_brains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4nm2akwHMas"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from numbasom.core import lattice_closest_vectors\n",
        "from analysis.som import SOM\n",
        "from utils import save_output_to_pickle\n",
        "from models.olfaction.olfaction import Olfaction\n",
        "from models.vision.vision import Vision\n",
        "from models.audio.audio import Audio\n",
        "from models.touch.touch import Touch\n",
        "from models.memory.memory import Memory\n",
        "import argparse\n",
        "\n",
        "from config import Args\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--fast', action='store_true', help='Use smaller SOM size (5,5)')\n",
        "    cmd_args = parser.parse_args()\n",
        "\n",
        "    args = Args()\n",
        "    args.experiment_name = \"main_results\"\n",
        "    \n",
        "    if cmd_args.fast:\n",
        "        args.experiment_name = \"main_results_fast\"\n",
        "        args.som_size = (5,5)\n",
        "\n",
        "    for modality in ([Olfaction(args), Vision(args), Audio(args), Touch(args), Memory(args)]):\n",
        "        print (\"\\n\", modality.modality, pd.Timestamp.now())\n",
        "        if modality.modality == 'audio':\n",
        "            args.setup_models = True # always setup, bc weights too large to save in repo\n",
        "        else:\n",
        "            args.setup_models = False # otherwise setup with setup.py\n",
        "\n",
        "        # 1. Train/Download Model\n",
        "        modality.setup_model()\n",
        "        modality.setup_som()\n",
        "\n",
        "        # 2. Get activations for each patch\n",
        "        patches = modality.get_patches()\n",
        "        activation_list = []\n",
        "        for p in tqdm(patches):\n",
        "            p, static = modality.generate_static(p)\n",
        "            activation = modality.calculate_activations(static)\n",
        "            activation_list.append([p, activation])\n",
        "\n",
        "        # 3. Fit SOM\n",
        "        x_mat = torch.stack([x[1] for x in activation_list]).numpy()\n",
        "        som = modality.initialize_som(SOM)\n",
        "        lattice = som.train(x_mat, num_iterations=args.som_epochs, initialize=args.som_init, normalize=False, start_lrate=args.som_lr)\n",
        "        \n",
        "        # 4. Get coordinates for each BMU\n",
        "        coordinate_list = [x[0] for x in activation_list]\n",
        "        closest = lattice_closest_vectors(x_mat, lattice, additional_list=coordinate_list)\n",
        "\n",
        "        # 5. Save\n",
        "        output = {\"closest\": closest, \n",
        "                \"coord_map\": coordinate_list,\n",
        "                \"x_range\": (0, max([x[0][0] for x in activation_list])),\n",
        "                \"y_range\": (0, max([x[0][1] for x in activation_list])),\n",
        "                \"lattice\": lattice,\n",
        "                \"som\": None,\n",
        "                \"samples\": modality.sample_data,\n",
        "                \"modality\": modality.modality,\n",
        "                \"args\": args,\n",
        "                \"activations\": activation_list}\n",
        "\n",
        "        save_output_to_pickle(output, args.experiment_name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "babylm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
