{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkielo3/mammalian_brains/blob/main/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JR3Ptn-SFYRJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy pandas scipy scikit-learn matplotlib seaborn pillow ipykernel\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q altair datasets\n",
        "!pip install -q numba numbasom==0.0.5\n",
        "!pip install -q tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mkielo3/mammalian_brains.git\n",
        "%cd mammalian_brains"
      ],
      "metadata": {
        "id": "ydHq9IbUGuwk",
        "outputId": "df77c198-69f2-4026-fd23-2a54e87390dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mammalian_brains'...\n",
            "remote: Enumerating objects: 253, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 253 (delta 6), reused 19 (delta 4), pack-reused 222 (from 1)\u001b[K\n",
            "Receiving objects: 100% (253/253), 97.68 MiB | 33.19 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/mammalian_brains\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from numbasom.core import lattice_closest_vectors\n",
        "from analysis.som import SOM\n",
        "from utils import save_output_to_pickle\n",
        "\n",
        "from models.olfaction.olfaction import Olfaction\n",
        "from models.vision.vision import Vision\n",
        "from models.audio.audio import Audio\n",
        "from models.touch.touch import Touch\n",
        "from models.memory.memory import Memory\n",
        "\n",
        "from config import Args"
      ],
      "metadata": {
        "id": "ghE49I9vFeic"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAUFSqOlFYRL",
        "outputId": "0317985f-63ad-4a3d-9b81-576085c603fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mammalian_brains/models/olfaction/olfaction.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"{self.project_path}/models/olfaction/saved_data/model.pt\", map_location=torch.device('cpu')))\n",
            "/content/mammalian_brains/models/olfaction/olfaction.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(f\"{self.project_path}/models/olfaction/saved_data/val_dataset.pt\", map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " olfaction 2025-02-07 06:32:14.350133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4096/4096 [00:02<00:00, 1463.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SOM with Random\n",
            "Done Init\n",
            "SOM training took: 14.742145 seconds.\n",
            "Finding closest data points took: 0.544560 seconds.\n",
            "Saved output to: output/main_results/olfaction_20250207_063232.pkl\n",
            "\n",
            " vision 2025-02-07 06:32:32.882206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            " 48%|████▊     | 21.5M/44.7M [00:00<00:00, 104MB/s]"
          ]
        }
      ],
      "source": [
        "    args = Args()\n",
        "    args.setup_models = False # Set to True if first ever time running\n",
        "    args.experiment_name = \"main_results\"\n",
        "\n",
        "    for modality in ([Olfaction(args), Vision(args), Audio(args), Touch(args), Memory(args)]):\n",
        "        print (\"\\n\", modality.modality, pd.Timestamp.now())\n",
        "\n",
        "        # 1. Train/Download Model\n",
        "        modality.setup_model()\n",
        "        modality.setup_som()\n",
        "\n",
        "        # 2. Get activations for each patch\n",
        "        patches = modality.get_patches()\n",
        "        activation_list = []\n",
        "        for p in tqdm(patches):\n",
        "            p, static = modality.generate_static(p)\n",
        "            activation = modality.calculate_activations(static)\n",
        "            activation_list.append([p, activation])\n",
        "\n",
        "        # 3. Fit SOM\n",
        "        x_mat = torch.stack([x[1] for x in activation_list]).numpy()\n",
        "        som = modality.initialize_som(SOM)\n",
        "        lattice = som.train(x_mat, num_iterations=args.som_epochs, initialize=args.som_init, normalize=False, start_lrate=args.som_lr)\n",
        "\n",
        "        # 4. Get coordinates for each BMU\n",
        "        coordinate_list = [x[0] for x in activation_list]\n",
        "        closest = lattice_closest_vectors(x_mat, lattice, additional_list=coordinate_list)\n",
        "\n",
        "        # 5. Save\n",
        "        output = {\"closest\": closest,\n",
        "                \"coord_map\": coordinate_list,\n",
        "                \"x_range\": (0, max([x[0][0] for x in activation_list])),\n",
        "                \"y_range\": (0, max([x[0][1] for x in activation_list])),\n",
        "                \"lattice\": lattice,\n",
        "                \"som\": None,\n",
        "                \"samples\": modality.sample_data,\n",
        "                \"modality\": modality.modality,\n",
        "                \"args\": args,\n",
        "                \"activations\": activation_list\n",
        "                }\n",
        "\n",
        "        save_output_to_pickle(output, args.experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "376YdLalFYRL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "imgs = torch.load(f\"models/vision/saved_data/processed_images.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCq7POTeFYRL",
        "outputId": "7673c78a-0ddb-4363-fa5c-abb3eee5e40d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100, 3, 224, 224])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoSRrfFTFYRM"
      },
      "outputs": [],
      "source": [
        "imgs_subset = imgs[:50].clone().contiguous()\n",
        "torch.save(imgs_subset, \"models/vision/saved_data/processed_images.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ongww_xAFYRM",
        "outputId": "f37ad813-dcf8-48ac-a921-d62c34e1a34d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([50, 3, 224, 224])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "imgs_subset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOX-3WWbFYRM"
      },
      "outputs": [],
      "source": [
        "data = torch.load(f\"models/olfaction/saved_data/val_dataset.pt\", map_location='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFgtWcxTFYRM"
      },
      "outputs": [],
      "source": [
        "imgs_subset = data.clone().contiguous()\n",
        "torch.save(imgs_subset, \"models/olfaction/saved_data/val_dataset.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15Jr2K6OFYRM",
        "outputId": "6f3b5bff-9316-4454-ed6b-964ee01301f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 3280\n",
            "-rw-r--r--  1 matthewkielo  staff     2745 Feb  7 00:53 README.md\n",
            "drwxr-xr-x  6 matthewkielo  staff      192 Feb  7 00:53 \u001b[34manalysis\u001b[m\u001b[m\n",
            "-rw-r--r--  1 matthewkielo  staff     2110 Feb  7 00:57 config.py\n",
            "-rw-r--r--  1 matthewkielo  staff  1639511 Feb  7 01:02 foobar.pt\n",
            "-rw-r--r--  1 matthewkielo  staff     2157 Feb  7 00:53 main.py\n",
            "drwxr-xr-x  9 matthewkielo  staff      288 Feb  7 00:53 \u001b[34mmodels\u001b[m\u001b[m\n",
            "-rw-r--r--  1 matthewkielo  staff     4260 Feb  7 00:57 quickstart.ipynb\n",
            "-rw-r--r--  1 matthewkielo  staff     7420 Feb  7 00:53 requirements.txt\n",
            "-rw-r--r--  1 matthewkielo  staff      689 Feb  7 00:53 setup.py\n",
            "-rw-r--r--  1 matthewkielo  staff      507 Feb  7 00:53 utils.py\n"
          ]
        }
      ],
      "source": [
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4wSCvNtFYRM"
      },
      "outputs": [],
      "source": [
        "1639511\n",
        "163841200"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "babylm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}