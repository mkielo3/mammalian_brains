{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from numbasom.core import lattice_closest_vectors\n",
    "from analysis.som import SOM\n",
    "from utils import save_output_to_pickle\n",
    "\n",
    "from models.olfaction.olfaction import Olfaction\n",
    "from models.vision.vision import Vision\n",
    "from models.audio.audio import Audio\n",
    "from models.touch.touch import Touch\n",
    "from models.memory.memory import Memory\n",
    "\n",
    "from config import Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [0.01, 0.1, 0.2]:\n",
    "    args = Args()\n",
    "    args.setup_models = False # Set to True if first ever time running\n",
    "    args.experiment_name = \"main_results_lr_sweep\"\n",
    "    args.som_lr = lr\n",
    "\n",
    "    for modality in ([Olfaction(args), Vision(args), Audio(args), Touch(args), Memory(args)]):\n",
    "        print (\"\\n\", modality.modality, pd.Timestamp.now())\n",
    "        \n",
    "        # 1. Train/Download Model\n",
    "        modality.setup_model()\n",
    "        modality.setup_som()\n",
    "\n",
    "        # 2. Get activations for each patch\n",
    "        patches = modality.get_patches()\n",
    "        activation_list = []\n",
    "        for p in tqdm(patches):\n",
    "            p, static = modality.generate_static(p)\n",
    "            activation = modality.calculate_activations(static)\n",
    "            activation_list.append([p, activation])\n",
    "\n",
    "        # 3. Fit SOM\n",
    "        x_mat = torch.stack([x[1] for x in activation_list]).numpy()\n",
    "        som = modality.initialize_som(SOM)\n",
    "        lattice = som.train(x_mat, num_iterations=args.som_epochs, initialize=args.som_init, normalize=False, start_lrate=args.som_lr)\n",
    "        \n",
    "        # 4. Get coordinates for each BMU\n",
    "        coordinate_list = [x[0] for x in activation_list]\n",
    "        closest = lattice_closest_vectors(x_mat, lattice, additional_list=coordinate_list)\n",
    "\n",
    "        # 5. Save\n",
    "        output = {\"closest\": closest, \n",
    "                \"coord_map\": coordinate_list,\n",
    "                \"x_range\": (0, max([x[0][0] for x in activation_list])),\n",
    "                \"y_range\": (0, max([x[0][1] for x in activation_list])),\n",
    "                \"lattice\": lattice,\n",
    "                \"som\": None,\n",
    "                \"samples\": modality.sample_data,\n",
    "                \"modality\": modality.modality,\n",
    "                \"args\": args,\n",
    "                \"activations\": activation_list\n",
    "                }\n",
    "\n",
    "        save_output_to_pickle(output, args.experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "imgs = torch.load(f\"models/vision/saved_data/processed_images.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_subset = imgs[:50].clone().contiguous()\n",
    "torch.save(imgs_subset, \"models/vision/saved_data/processed_images.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imgs_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(f\"models/olfaction/saved_data/val_dataset.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_subset = data.clone().contiguous()\n",
    "torch.save(imgs_subset, \"models/olfaction/saved_data/val_dataset.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3280\n",
      "-rw-r--r--  1 matthewkielo  staff     2745 Feb  7 00:53 README.md\n",
      "drwxr-xr-x  6 matthewkielo  staff      192 Feb  7 00:53 \u001b[34manalysis\u001b[m\u001b[m\n",
      "-rw-r--r--  1 matthewkielo  staff     2110 Feb  7 00:57 config.py\n",
      "-rw-r--r--  1 matthewkielo  staff  1639511 Feb  7 01:02 foobar.pt\n",
      "-rw-r--r--  1 matthewkielo  staff     2157 Feb  7 00:53 main.py\n",
      "drwxr-xr-x  9 matthewkielo  staff      288 Feb  7 00:53 \u001b[34mmodels\u001b[m\u001b[m\n",
      "-rw-r--r--  1 matthewkielo  staff     4260 Feb  7 00:57 quickstart.ipynb\n",
      "-rw-r--r--  1 matthewkielo  staff     7420 Feb  7 00:53 requirements.txt\n",
      "-rw-r--r--  1 matthewkielo  staff      689 Feb  7 00:53 setup.py\n",
      "-rw-r--r--  1 matthewkielo  staff      507 Feb  7 00:53 utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1639511\n",
    "163841200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babylm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
